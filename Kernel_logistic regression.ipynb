{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Kernelized) Logistic regression\n",
    "\n",
    "KIAS CAC Summer School 2021\n",
    "\n",
    "Dates: 2021-7-12\n",
    "\n",
    "Author: Yung-Kyun Noh\n",
    "\n",
    "Department of Computer Science, Hanyang University / School of Computational Sciences, KIAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(ax, title_str='Data'):\n",
    "    \n",
    "    # Scatter data points in 2-dimensional space\n",
    "    ax.scatter(data1[:,0], data1[:,1], label='class 1', c='red', alpha=.3)\n",
    "    ax.scatter(data2[:,0], data2[:,1], label='class 2', marker='^', c='blue', alpha=.3)\n",
    "    # set a title and labels\n",
    "    ax.set_title(title_str)\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_state_linear(ax, wval, bval, title_str='Data'):\n",
    "    # function for scattering data and drawing classification boundary\n",
    "    # wx - b > 0 or  wx - b < 0\n",
    "\n",
    "    plot_data(ax, title_str)\n",
    "    \n",
    "    [x1min,x1max,x2min,x2max] = ax.axis()\n",
    "    x1vals = np.arange(x1min,x1max,0.1)\n",
    "    ax.plot(x1vals, (-wval[0]*x1vals + bval)/wval[1], 'k')\n",
    "    ax.axis([x1min,x1max,x2min,x2max])\n",
    "    ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_kernel_matrix(data1, data2, sig_param=0.1):\n",
    "    G1mat = np.diag(np.matmul(data1, data1.T))\n",
    "    G2mat = np.diag(np.matmul(data2, data2.T))\n",
    "    Gmat = np.matmul(data1, data2.T)\n",
    "    \n",
    "    Dists = G1mat[:, np.newaxis] + G2mat - 2*Gmat\n",
    "    Kmat = np.exp(-Dists/sig_param)\n",
    "    return Kmat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_state_kernel(ax, alphas, title_str='Data'):\n",
    "    # function for scattering data and drawing classification boundary\n",
    "\n",
    "    spacing = 0.025\n",
    "\n",
    "    meshx = np.arange(-0.6, 1.7, spacing)\n",
    "    meshy = np.arange(-1.8, 1.5, spacing)\n",
    "    meshX, meshY = np.meshgrid(meshx, meshy)\n",
    "    contourShape = meshX.shape\n",
    "\n",
    "    [x1min,x1max,x2min,x2max] = ax.axis()\n",
    "\n",
    "    meshgridData = np.concatenate((meshX.reshape((-1,1)), meshY.reshape((-1,1))), axis=1)\n",
    "\n",
    "    Kmat = get_gaussian_kernel_matrix(meshgridData, np.concatenate([data1, data2], axis=0))\n",
    "    \n",
    "    ts = np.matmul(Kmat, alphas)\n",
    "    Zs = logistic_function(ts).reshape(contourShape)\n",
    "\n",
    "    ax.grid()\n",
    "\n",
    "    levels = [0,.5,1]\n",
    "    CS = plt.contourf(meshX, meshY, Zs, levels, colors=('r', 'g'))\n",
    "\n",
    "    plot_data(ax, title_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(t):\n",
    "    # example: logistic_function(np.array([0,1,2]))\n",
    "    \n",
    "    ret_val = 1/(1 + np.exp(-t))\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(labels, fs):\n",
    "    loss_val = np.sum(labels*np.log(fs) + (1 - labels)*np.log(1 - fs))\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mixture of Gaussians (class 1 & class 2)\n",
    "dim = 2\n",
    "datanum1_1 = 10\n",
    "datanum1_2 = 5\n",
    "datanum2_1 = 10\n",
    "datanum2_2 = 5\n",
    "datanum1 = datanum1_1 + datanum1_2\n",
    "datanum2 = datanum2_1 + datanum2_2\n",
    "mean1_1 = np.array([0, 0])\n",
    "mean1_2 = np.array([1, 1])\n",
    "mean2_1 = np.array([.5, .5])\n",
    "mean2_2 = np.array([0, -1])\n",
    "cov1_1 = np.array([[.01,0],[0,.2]])\n",
    "cov1_2 = np.array([[.1,.02],[.02,.1]])\n",
    "cov2_1 = np.array([[.02,0],[0,.2]])\n",
    "cov2_2 = np.array([[.1,-.01],[-.01,.05]])\n",
    "data1_1 = np.random.multivariate_normal(mean1_1, cov1_1, datanum1_1)\n",
    "data1_2 = np.random.multivariate_normal(mean1_2, cov1_2, datanum1_2)\n",
    "data2_1 = np.random.multivariate_normal(mean2_1, cov2_1, datanum2_1)\n",
    "data2_2 = np.random.multivariate_normal(mean2_2, cov2_2, datanum2_2)\n",
    "data1 = np.concatenate((data1_1, data1_2), axis=0)\n",
    "data2 = np.concatenate((data2_1, data2_2), axis=0)\n",
    "\n",
    "tstdatanum1_1 = 100\n",
    "tstdatanum1_2 = 100\n",
    "tstdatanum2_1 = 100\n",
    "tstdatanum2_2 = 100\n",
    "tstdatanum1 = tstdatanum1_1 + tstdatanum1_2\n",
    "tstdatanum2 = tstdatanum2_1 + tstdatanum2_2\n",
    "tstdata1_1 = np.random.multivariate_normal(mean1_1, cov1_1, tstdatanum1_1)\n",
    "tstdata1_2 = np.random.multivariate_normal(mean1_2, cov1_2, tstdatanum1_2)\n",
    "tstdata2_1 = np.random.multivariate_normal(mean2_1, cov2_1, tstdatanum2_1)\n",
    "tstdata2_2 = np.random.multivariate_normal(mean2_2, cov2_2, tstdatanum2_2)\n",
    "tstdata1 = np.concatenate((tstdata1_1, tstdata1_2), axis=0)\n",
    "tstdata2 = np.concatenate((tstdata2_1, tstdata2_2), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# draw_state_linear(ax)\n",
    "plot_data(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with logistic regression (without kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "w_init = np.random.normal(0,1,dim)\n",
    "b_init = np.random.normal(0,1,1)\n",
    "\n",
    "# extended w: [w, -b]\n",
    "extw = np.array([np.concatenate((w_init, -b_init))])\n",
    "# data with '1' is appended: [X, 1]\n",
    "extX = np.concatenate((np.concatenate((data1, data2), axis=0), \\\n",
    "                       np.ones([datanum1 + datanum2, 1])), axis=1)\n",
    "labels = np.concatenate((np.ones(datanum1), np.zeros(datanum2)))  # label of class 1: 1, label of class 2: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w(extw, extX, labels, num_update=100, step_size=0.001, reg_const=1):\n",
    "    fig, ax = plt.subplots()\n",
    "    draw_state_linear(ax, extw[0,0:2], -extw[0,2], 'Updated boundary')\n",
    "\n",
    "    objective_history = []\n",
    "    for i in range(num_update):\n",
    "        ts = np.matmul(extX, extw.T)  # w^TX\n",
    "        fs = logistic_function(ts)\n",
    "        extw = extw + step_size*(np.matmul(np.array([labels]) - fs.T, extX) - reg_const*extw)\n",
    "        objective_history.append(get_loss(labels, fs.T[0]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    draw_state_linear(ax, extw[0,0:2], -extw[0,2], 'Updated boundary')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(objective_history)\n",
    "    ax.set_title(\"objective function\")\n",
    "    \n",
    "    return extw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extw = update_w(extw, extX, labels, num_update=100, step_size=0.01, reg_const=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional 100 updates\n",
    "extw = update_w(extw, extX, labels, num_update=100, step_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After enough updates\n",
    "extw = update_w(extw, extX, labels, num_update=1000, step_size=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with '1' is appended: [X, 1]\n",
    "extTstX = np.concatenate((np.concatenate((tstdata1, tstdata2), axis=0), \\\n",
    "                       np.ones([tstdatanum1 + tstdatanum2, 1])), axis=1)\n",
    "tstlabels = np.concatenate((np.ones(tstdatanum1), np.zeros(tstdatanum2)))\n",
    "\n",
    "ts = np.matmul(extTstX, extw.T)  # w^TX\n",
    "err_rate = np.sum(np.abs((ts.T > 0) - np.array([tstlabels])))/(tstdatanum1 + tstdatanum2)\n",
    "print(err_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with logistic regression (with kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "\\frac{dL}{d\\alpha_k} = \\sum_{i = 1}^N (y_i - f(\\mathbf{x}_i)) k(\\mathbf{x}_k, \\mathbf{x}_i)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha(alphas, X, labels, num_update=100, step_size=0.001, reg_const=1):\n",
    "    fig, ax = plt.subplots()\n",
    "    draw_state_kernel(ax, alphas, 'Updated boundary')\n",
    "\n",
    "    objective_history = []\n",
    "    for i in range(num_update):\n",
    "        Kmat = get_gaussian_kernel_matrix(X, X)\n",
    "        ts = np.matmul(alphas, Kmat)  # K*alpha\n",
    "        fs = logistic_function(ts)\n",
    "        alphas = alphas + step_size*(np.matmul(labels - fs, Kmat) - reg_const*np.matmul(alphas, Kmat))\n",
    "        objective_history.append(get_loss(labels, fs))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    draw_state_kernel(ax, alphas, 'Updated boundary')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(objective_history)\n",
    "    ax.set_title(\"objective function\")\n",
    "    \n",
    "    return alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha initialize\n",
    "alphas = np.random.normal(0, 0.01, datanum1 + datanum2)\n",
    "labels = np.concatenate((np.ones(datanum1), np.zeros(datanum2)))  # label of class 1: 1, label of class 2: 0\n",
    "X = np.concatenate([data1, data2], axis=0)\n",
    "alphas = update_alpha(alphas, X, labels, num_update=100, step_size=0.001, reg_const=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_alpha(alphas, X, labels, num_update=1000, step_size=0.001, reg_const=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with '1' is appended: [X, 1]\n",
    "tstX = (np.concatenate([tstdata1, tstdata2], axis=0))\n",
    "tstlabels = np.concatenate([np.ones(tstdatanum1), np.zeros(tstdatanum2)], axis=0)\n",
    "\n",
    "Kmat = get_gaussian_kernel_matrix(X, tstX)\n",
    "ts = np.matmul(alphas, Kmat)  # K*alpha\n",
    "err_rate = np.sum(np.abs((ts.T > 0) - np.array([tstlabels])))/(tstdatanum1 + tstdatanum2)\n",
    "print(err_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
